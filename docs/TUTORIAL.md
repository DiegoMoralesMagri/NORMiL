# Tutorial NORMiL - De Z√©ro √† Expert


**Date** : Novembre 2025
**Auteur** : Diego Morales Magri

---

## Apprenez NORMiL par la pratique

---

## Table des Mati√®res

1. [Niveau 1 : D√©butant](#niveau-1--d√©butant)
2. [Niveau 2 : Interm√©diaire](#niveau-2--interm√©diaire)
3. [Niveau 3 : Avanc√©](#niveau-3--avanc√©)
4. [Niveau 4 : Expert](#niveau-4--expert)
5. [Projets Complets](#projets-complets)

---

## Niveau 1 : D√©butant

### Le√ßon 1.1 : Premier Programme

**Objectif** : Afficher du texte et faire des calculs simples

```normil
fn main() {
    print("Bonjour NORMiL!")
  
    let x: int = 10
    let y: int = 20
    let somme: int = x + y
  
    print("La somme est:")
    print(somme)
}
```

**Exercice** : Modifiez ce programme pour calculer la diff√©rence et le produit.

---

### Le√ßon 1.2 : Variables et Types

**Objectif** : Comprendre les types de donn√©es

```normil
fn main() {
    // Types de base avec annotations
    let age: int = 25
    let temperature: float = 36.6
    let nom: str = "Alice"
    let actif: bool = true
  
    print("Informations:")
    print(nom)
    print(age)
    print(temperature)
    print(actif)
}
```

**Exercice** : Cr√©ez une fiche d'identit√© avec 5 variables diff√©rentes.

---

### Le√ßon 1.2b : Inf√©rence de Types (‚ú® Phase 3.1)

**Objectif** : Laisser NORMiL d√©duire les types automatiquement

```normil
fn main() {
    // Inf√©rence automatique - pas besoin de sp√©cifier le type!
    let age = 25              // D√©duit: int
    let temperature = 36.6    // D√©duit: float
    let nom = "Alice"         // D√©duit: str
    let actif = true          // D√©duit: bool
  
    // M√™me avec des expressions
    let somme = 10 + 20       // D√©duit: int
    let moyenne = 10.5 / 2.0  // D√©duit: float
    let message = "Bonjour"   // D√©duit: str
  
    print("L'inf√©rence fonctionne!")
    print(age)
    print(somme)
}
```

**Points cl√©s** :

- ‚úÖ `let x = 42` au lieu de `let x: int = 42`
- ‚úÖ Fonctionne avec literals, expressions, retours de fonction
- ‚úÖ D√©duction intelligente : bool ‚Üí int ‚Üí float ‚Üí str ‚Üí Vec
- ‚úÖ Compatible avec vecteurs et fonctions

**Exercice** : R√©√©crivez l'exercice 1.2 sans aucune annotation de type.

---

### Le√ßon 1.3 : Fonctions Simples

**Objectif** : Cr√©er et appeler des fonctions

```normil
fn double(x: int) -> int {
    return x * 2
}

fn triple(x: int) -> int {
    return x * 3
}

fn main() {
    let nombre: int = 5
  
    let d = double(nombre)
    let t = triple(nombre)
  
    print("Double:")
    print(d)
    print("Triple:")
    print(t)
}
```

**Exercice** : Cr√©ez une fonction `quadruple` et une fonction `carre`.

---

### Le√ßon 1.4 : Conditions

**Objectif** : Utiliser if-else

```normil
fn evaluer_note(note: int) -> str {
    if note >= 90 {
        return "Excellent"
    } else if note >= 70 {
        return "Bien"
    } else if note >= 50 {
        return "Passable"
    } else {
        return "Insuffisant"
    }
}

fn main() {
    print(evaluer_note(95))
    print(evaluer_note(75))
    print(evaluer_note(45))
}
```

**Exercice** : Cr√©ez une fonction qui classe une temp√©rature en "chaud", "ti√®de", "froid".

---

### Le√ßon 1.5 : Boucles

**Objectif** : R√©p√©ter des actions

```normil
fn compter_jusque(n: int) {
    let i = 0
    while i < n {
        print(i)
        i = i + 1
    }
}

fn compter_range(debut: int, fin: int) {
    for i in range(debut, fin) {
        print(i)
    }
}

fn main() {
    print("While:")
    compter_jusque(5)
  
    print("For:")
    compter_range(0, 5)
}
```

**Exercice** : Cr√©ez une fonction qui calcule la somme de 1 √† N.

---

### Le√ßon 1.6 : Op√©rations sur Cha√Ænes (‚ú® Phase 3.3)

**Objectif** : Manipuler des cha√Ænes de caract√®res

```normil
fn main() {
    // Concat√©nation avec l'op√©rateur +
    let prenom = "Alice"
    let nom = "Dupont"
    let nom_complet = prenom + " " + nom
    print(nom_complet)  // "Alice Dupont"
  
    // Conversion vers string
    let age = 25
    let message = "J'ai " + to_string(age) + " ans"
    print(message)  // "J'ai 25 ans"
  
    // Primitives string
    let texte = "NORMiL"
    print(string_length(texte))        // 6
    print(string_upper(texte))         // "NORMIL"
    print(string_lower(texte))         // "normil"
  
    // Manipulation
    let phrase = "Hello World"
    print(string_substring(phrase, 0, 5))   // "Hello"
    print(string_replace(phrase, "World", "NORMiL"))  // "Hello NORMiL"
    print(string_contains(phrase, "World")) // true
  
    // R√©p√©tition
    print(string_repeat("Ha", 3))  // "HaHaHa"
}
```

**Primitives disponibles** :

- `string_length(s: str) -> int` - Longueur de la cha√Æne
- `string_upper(s: str) -> str` - En majuscules
- `string_lower(s: str) -> str` - En minuscules
- `string_substring(s: str, start: int, end: int) -> str` - Sous-cha√Æne
- `string_split(s: str, sep: str) -> str` - D√©coupe (retourne premier √©l√©ment)
- `string_join(items: str, sep: str) -> str` - Joint avec s√©parateur
- `string_replace(s: str, old: str, new: str) -> str` - Remplace
- `string_contains(s: str, sub: str) -> bool` - Contient?
- `string_startswith(s: str, prefix: str) -> bool` - Commence par?
- `string_endswith(s: str, suffix: str) -> bool` - Finit par?
- `string_trim(s: str) -> str` - Enl√®ve espaces d√©but/fin
- `string_repeat(s: str, n: int) -> str` - R√©p√®te n fois
- `string_char_at(s: str, index: int) -> str` - Caract√®re √† l'index
- `string_index_of(s: str, sub: str) -> int` - Position de sous-cha√Æne

**Exercice** : Cr√©ez une fonction qui formate un nom en "NOM, Pr√©nom" (majuscules).

---

## Niveau 2 : Interm√©diaire

### Le√ßon 2.1 : Vecteurs de Base

**Objectif** : Manipuler des vecteurs

```normil
fn main() {
    // Cr√©ation de vecteurs - avec inf√©rence de type!
    let v1 = zeros(dim: 64)
    let v2 = ones(dim: 64)
    let v3 = fill(dim: 64, value: 0.5)
    let v4 = random(dim: 64, mean: 0.0, std: 1.0)
  
    // Op√©rations
    let somme = vec_add(v1, v2)
    let produit = vec_mul(v2, v3)
    let double_v = scale(v2, 2.0)
  
    // M√©triques
    print("Norme de v4:")
    print(norm(v4))
  
    print("Norme de somme:")
    print(norm(somme))
}
```

**Exercice** : Cr√©ez 3 vecteurs et calculez leur moyenne.

---

### Le√ßon 2.1b : Modules et Imports (‚ú® Phase 3.2)

**Objectif** : R√©utiliser du code avec le syst√®me de modules

**Cr√©ez un module** : `modules/math_utils.nor`

```normil
fn abs(x: float) -> float {
    if x < 0.0 {
        return -x
    } else {
        return x
    }
}

fn max(a: float, b: float) -> float {
    if a > b {
        return a
    } else {
        return b
    }
}

fn min(a: float, b: float) -> float {
    if a < b {
        return a
    } else {
        return b
    }
}
```

**Utilisez le module** : `main.nor`

```normil
import math_utils

fn main() {
    // Appel de fonctions du module
    let valeur = math_utils.abs(-42.0)
    print(valeur)  // 42.0
  
    let maximum = math_utils.max(10.0, 25.0)
    print(maximum)  // 25.0
}
```

**Avec alias** :

```normil
import math_utils as math

fn main() {
    print(math.abs(-42.0))
    print(math.max(10.0, 25.0))
}
```

**Modules disponibles** :

- `modules/math.nor` - Fonctions math√©matiques (abs, max, min, clamp)
- `modules/vectors.nor` - Op√©rations vectorielles avanc√©es

**Points cl√©s** :

- ‚úÖ Modules dans le dossier `modules/`
- ‚úÖ Import avec ou sans alias
- ‚úÖ Acc√®s via `module.fonction()`
- ‚úÖ Caching automatique (module charg√© une seule fois)
- ‚úÖ Scopes isol√©s entre modules

**Exercice** : Cr√©ez un module `string_utils.nor` avec 3 fonctions utiles.

---

### Le√ßon 2.1c : Interop√©rabilit√© Python (üöÄ Phase 4.1)

**Objectif** : Utiliser des biblioth√®ques Python depuis NORMiL

NORMiL peut importer et utiliser **n'importe quel module Python** ! Cela vous donne acc√®s √† tout l'√©cosyst√®me Python : NumPy, SciPy, pandas, et plus encore.

**Import de modules Python standards** :

```normil
import math

fn main() {
    // Acc√®s aux constantes Python
    let pi = math.pi
    print(pi)  // 3.141592653589793
  
    let e = math.e
    print(e)   // 2.718281828459045
  
    // Appel de fonctions Python
    let racine = math.sqrt(16.0)
    print(racine)  // 4.0
  
    let puissance = math.pow(2.0, 3.0)
    print(puissance)  // 8.0
}
```

**Avec alias** :

```normil
import math as m

fn main() {
    let aire_cercle = m.pi * 5.0 * 5.0
    print(aire_cercle)  // 78.53981633974483
}
```

**Fonctions trigonom√©triques** :

```normil
import math

fn main() {
    let angle = math.pi / 4.0  // 45 degr√©s en radians
  
    let sin_val = math.sin(angle)
    print(sin_val)  // 0.7071...
  
    let cos_val = math.cos(angle)
    print(cos_val)  // 0.7071...
  
    let tan_val = math.tan(angle)
    print(tan_val)  // 1.0
}
```

**Module random** :

```normil
import random

fn main() {
    // Fixer la seed pour reproductibilit√©
    random.seed(42)
  
    // Nombre al√©atoire entre 0 et 1
    let val = random.random()
    print(val)
  
    // Entier al√©atoire
    let dice = random.randint(1, 6)
    print(dice)
}
```

**Utilisation dans des fonctions** :

```normil
import math

fn aire_cercle(rayon: float) -> float {
    return math.pi * rayon * rayon
}

fn volume_sphere(rayon: float) -> float {
    return (4.0 / 3.0) * math.pi * rayon * rayon * rayon
}

fn main() {
    let r = 5.0
  
    print("Aire du cercle:")
    print(aire_cercle(r))
  
    print("Volume de la sph√®re:")
    print(volume_sphere(r))
}
```

**M√©langer modules NORMiL et Python** :

```normil
import math           // Module Python
import mathutils      // Module NORMiL

fn main() {
    // Fonctions Python
    let sqrt_val = math.sqrt(25.0)
    let sin_val = math.sin(math.pi)
  
    // Fonctions NORMiL
    let abs_val = mathutils.abs(-42)
    let max_val = mathutils.max(10, 20)
  
    print(sqrt_val)
    print(abs_val)
}
```

**Appels imbriqu√©s** :

```normil
import math

fn main() {
    // Les appels Python peuvent √™tre imbriqu√©s
    let resultat = math.sqrt(math.pow(3.0, 2.0) + math.pow(4.0, 2.0))
    print(resultat)  // 5.0 (th√©or√®me de Pythagore)
  
    // Dans des expressions complexes
    let aire = math.pi * math.pow(math.sqrt(100.0), 2.0)
    print(aire)  // 314.159...
}
```

**Points cl√©s** :

- ‚úÖ Import transparent : `import math`, `import random`, `import sys`, etc.
- ‚úÖ Acc√®s aux constantes : `math.pi`, `math.e`, `math.inf`
- ‚úÖ Appel de fonctions : `math.sqrt()`, `math.sin()`, `random.random()`
- ‚úÖ Alias support√©s : `import math as m`
- ‚úÖ D√©tection automatique : NORMiL cherche d'abord `.nor`, puis Python
- ‚úÖ Mix NORMiL/Python dans le m√™me code
- ‚úÖ Cache intelligent : module charg√© une seule fois

**Modules Python utiles** :

- `math` - Fonctions math√©matiques
- `random` - G√©n√©ration al√©atoire
- `datetime` - Manipulation de dates
- `json` - Parsing JSON
- `sys` - Informations syst√®me
- `os` - Op√©rations syst√®me
- `collections` - Structures de donn√©es
- (Et tous les autres modules Python disponibles !)

**Exercice** : Cr√©ez un programme qui utilise `math` et `random` pour g√©n√©rer des coordonn√©es al√©atoires dans un cercle unitaire.

---

### Le√ßon 2.1d : Objets Python (üöÄ Phase 4.4)

**Objectif** : Manipuler des objets et classes Python

NORMiL permet d'acc√©der aux objets Python, leurs m√©thodes et attributs de mani√®re native.

#### M√©thodes sur les Types Natifs

**M√©thodes sur les cha√Ænes** :

```normil
fn manipuler_texte() {
    let message = "bonjour le monde"
  
    // Conversion casse
    let upper = message.upper()
    print(upper)  // "BONJOUR LE MONDE"
  
    let lower = upper.lower()
    print(lower)  // "bonjour le monde"
  
    // Remplacement
    let nouveau = message.replace("monde", "NORMiL")
    print(nouveau)  // "bonjour le NORMiL"
  
    // D√©coupage
    let mots = message.split(" ")
    print(mots)  // ["bonjour", "le", "monde"]
  
    // Tests
    let commence = message.startswith("bonjour")
    print(commence)  // true
}
```

**M√©thodes sur les listes** :

```normil
fn manipuler_listes() {
    let nombres = [1, 2, 3]
  
    // Ajouter des √©l√©ments
    nombres.append(4)
    nombres.append(5)
    print(nombres)  // [1, 2, 3, 4, 5]
}
```

#### Cha√Ænage de M√©thodes

Les m√©thodes peuvent √™tre cha√Æn√©es :

```normil
fn chainer_methodes() {
    let texte = "  hello world  "
  
    // Cha√Æner strip() puis upper()
    let resultat = texte.strip().upper()
    print(resultat)  // "HELLO WORLD"
  
    // Cha√Ænes complexes
    let complexe = "  python rocks  "
        .strip()
        .replace("python", "NORMiL")
        .upper()
    print(complexe)  // "NORMIL ROCKS"
}
```

#### Instantiation de Classes Python

**Cr√©er des objets** :

```normil
import datetime

fn utiliser_datetime() {
    // Instantiation d'une classe Python
    let noel = datetime.datetime(2024, 12, 25)
  
    // Acc√®s aux attributs
    print(noel.year)   // 2024
    print(noel.month)  // 12
    print(noel.day)    // 25
  
    // Appel de m√©thodes
    let jour_semaine = noel.weekday()
    print(jour_semaine)  // 2 (mercredi, 0=lundi)
}
```

#### Acc√®s aux Attributs

Les attributs d'objets Python sont accessibles avec `.` :

```normil
import datetime

fn explorer_attributs() {
    let date = datetime.datetime(2024, 6, 15)
  
    // Attributs simples
    let annee = date.year
    let mois = date.month
    let jour = date.day
  
    print(annee)  // 2024
    print(mois)   // 6
    print(jour)   // 15
}
```

#### Exemples Pratiques

**Validation d'email** :

```normil
fn valider_email(email: str) -> bool {
    // Utiliser les m√©thodes Python
    let parties = email.split("@")
  
    if parties.length == 2 {
        let commence_ok = parties[0].length > 0
        let domaine_ok = parties[1].length > 0
        return commence_ok && domaine_ok
    }
  
    return false
}

fn main() {
    let email1 = "user@example.com"
    let email2 = "@example.com"
  
    print(valider_email(email1))  // true
    print(valider_email(email2))  // false
}
```

**Parsing CSV simple** :

```normil
fn parser_csv(ligne: str) -> [str] {
    return ligne.split(",")
}

fn main() {
    let entetes = "nom,prenom,age"
    let colonnes = parser_csv(entetes)
  
    // Transformer en titres
    let titre1 = colonnes[0].upper()
    let titre2 = colonnes[1].upper()
    let titre3 = colonnes[2].upper()
  
    print(titre1)  // "NOM"
    print(titre2)  // "PRENOM"
    print(titre3)  // "AGE"
}
```

**Calculs avec datetime** :

```normil
import datetime

fn analyser_dates() {
    let nouvel_an = datetime.datetime(2024, 1, 1)
    let mi_annee = datetime.datetime(2024, 6, 15)
    let fin_annee = datetime.datetime(2024, 12, 31)
  
    // Extraire informations
    print(nouvel_an.month)  // 1
    print(mi_annee.month)   // 6
    print(fin_annee.month)  // 12
  
    // Jour de la semaine
    print(nouvel_an.weekday())  // Lundi = 0
}
```

**Points cl√©s** :

- ‚úÖ M√©thodes natives : `.upper()`, `.lower()`, `.split()`, `.replace()`, etc.
- ‚úÖ Cha√Ænage : `text.strip().upper()`
- ‚úÖ Classes Python : `datetime.datetime(2024, 1, 1)`
- ‚úÖ Attributs objets : `date.year`, `date.month`, `date.day`
- ‚úÖ M√©thodes objets : `date.weekday()`
- ‚úÖ Types Python : str, list, datetime, etc.
- ‚úÖ Totalement transparent : comme du code NORMiL natif

**Limitations** :

- ‚ö†Ô∏è Pas de support kwargs Python (`func(x=1, y=2)`)
- ‚ö†Ô∏è Certains types complexes peuvent n√©cessiter des conversions
- ‚ö†Ô∏è Les exceptions Python sont propag√©es

**Exercice** : Cr√©ez un programme qui parse une date au format "JJ/MM/AAAA" en utilisant `.split()` et cr√©e un objet `datetime.datetime`.

---

### Le√ßon 2.2 : Arguments Nomm√©s

**Objectif** : Utiliser les arguments nomm√©s pour la clart√©

```normil
fn creer_vecteur_personnalise(
    taille: int,
    valeur_moyenne: float,
    deviation: float,
    normaliser: bool
) -> Vec {
    let v = random(dim: taille, mean: valeur_moyenne, std: deviation)
  
    if normaliser {
        return normalize(v)
    } else {
        return v
    }
}

fn main() {
    // Ordre des arguments clair et flexible
    let v1 = creer_vecteur_personnalise(
        taille: 128,
        valeur_moyenne: 1.0,
        deviation: 0.2,
        normaliser: true
    )
  
    let v2 = creer_vecteur_personnalise(
        normaliser: false,
        taille: 64,
        deviation: 0.5,
        valeur_moyenne: 0.0
    )
  
    print(norm(v1))  // Devrait √™tre ~1.0
    print(norm(v2))  // Norme variable
}
```

**Exercice** : Cr√©ez une fonction avec 5 param√®tres nomm√©s pour configurer un r√©seau.

---

### Le√ßon 2.3 : Pattern Matching - Bases

**Objectif** : Utiliser le pattern matching

```normil
fn classifier_nombre(n: int) -> str {
    match n {
        case 0 -> {
            return "zero"
        }
        case 1 -> {
            return "un"
        }
        case int(x) where x < 0 -> {
            return "negatif"
        }
        case int(x) where x > 100 -> {
            return "tres grand"
        }
        case _ -> {
            return "autre"
        }
    }
}

fn main() {
    print(classifier_nombre(0))      // "zero"
    print(classifier_nombre(-5))     // "negatif"
    print(classifier_nombre(150))    // "tres grand"
    print(classifier_nombre(42))     // "autre"
}
```

**Exercice** : Cr√©ez un classifier pour les jours de la semaine (1-7).

---

### Le√ßon 2.4 : Pattern Matching Avanc√©

**Objectif** : Combiner patterns et conditions

```normil
fn analyser_score(score: float) -> str {
    match score {
        case float(s) where s >= 0.95 -> {
            return "Exceptionnel"
        }
        case float(s) where s >= 0.85 -> {
            return "Excellent"
        }
        case float(s) where s >= 0.70 -> {
            return "Tres bien"
        }
        case float(s) where s >= 0.55 -> {
            return "Bien"
        }
        case float(s) where s >= 0.40 -> {
            return "Moyen"
        }
        case _ -> {
            return "Insuffisant"
        }
    }
}

fn main() {
    let scores = [0.99, 0.87, 0.65, 0.42, 0.20]
  
    for s in scores {
        print(analyser_score(s))
    }
}
```

**Exercice** : Cr√©ez un analyseur de temp√©rature avec 6 cat√©gories.

---

## Niveau 3 : Avanc√©

### Le√ßon 3.1 : Annotation @plastic

**Objectif** : Impl√©menter la plasticit√© neuronale

```normil
@plastic(rate: 0.01, mode: "hebbian")
fn apprentissage_hebbien(poids: Vec, entree: Vec) -> Vec {
    // Hebbian learning: "Neurons that fire together, wire together"
    let produit = vec_mul(poids, entree)
    let increment = scale(produit, 0.01)
    let nouveaux_poids = vec_add(poids, increment)
    return normalize(nouveaux_poids)
}

fn main() {
    let poids = random(dim: 64, mean: 0.0, std: 0.1)
    let signal = random(dim: 64, mean: 1.0, std: 0.2)
  
    print("Norme initiale:")
    print(norm(poids))
  
    // 10 √©tapes d'apprentissage
    for i in range(0, 10) {
        poids = apprentissage_hebbien(poids, signal)
    }
  
    print("Norme finale:")
    print(norm(poids))
}
```

**Exercice** : Testez avec diff√©rents `rate` (0.001, 0.01, 0.1) et observez.

---

### Le√ßon 3.2 : Modes de Plasticit√©

**Objectif** : Comparer les diff√©rents modes

```normil
@plastic(rate: 0.01, mode: "hebbian")
fn hebb(w: Vec, x: Vec) -> Vec {
    let delta = scale(vec_mul(w, x), 0.01)
    return vec_add(w, delta)
}

@plastic(rate: 0.01, mode: "anti_hebbian")
fn anti_hebb(w: Vec, x: Vec) -> Vec {
    let delta = scale(vec_mul(w, x), 0.01)
    return vec_sub(w, delta)
}

@plastic(rate: 0.01, mode: "stdp")
fn stdp(w: Vec, x: Vec) -> Vec {
    // STDP avec timing simul√©
    let timing_factor = scale(x, 0.8)  // Simule le timing
    let delta = scale(vec_mul(w, timing_factor), 0.01)
    return vec_add(w, delta)
}

fn main() {
    let w_init = random(dim: 32, mean: 0.0, std: 0.1)
    let signal = random(dim: 32, mean: 1.0, std: 0.1)
  
    let w_hebb = hebb(w_init, signal)
    let w_anti = anti_hebb(w_init, signal)
    let w_stdp = stdp(w_init, signal)
  
    print("Hebbian:")
    print(norm(w_hebb))
  
    print("Anti-Hebbian:")
    print(norm(w_anti))
  
    print("STDP:")
    print(norm(w_stdp))
}
```

**Exercice** : Ajoutez le mode "competitive" et comparez.

---

### Le√ßon 3.3 : Annotation @atomic

**Objectif** : Transactions avec rollback automatique

```normil
@atomic
fn mise_a_jour_securisee(valeur: int, increment: int) -> int {
    let temp = valeur + increment
  
    // Si erreur ici, rollback automatique
    if temp < 0 {
        return valeur  // Pas de changement
    }
  
    return temp
}

@atomic
fn normalisation_atomique(v: Vec) -> Vec {
    let n = norm(v)
  
    if n < 0.001 {
        // √âviter division par z√©ro
        return ones(dim: 64)
    }
  
    return normalize(v)
}

fn main() {
    let x = 10
    let y = mise_a_jour_securisee(x, 5)
    let z = mise_a_jour_securisee(x, -20)
  
    print(y)  // 15
    print(z)  // 10 (rollback)
  
    let v_zero = zeros(dim: 64)
    let v_safe = normalisation_atomique(v_zero)
    print(norm(v_safe))  // ~8.0 (norme de ones(64))
}
```

**Exercice** : Cr√©ez une fonction @atomic pour des transferts d'argent.

---

### Le√ßon 3.4 : Combinaison @atomic + @plastic

**Objectif** : Apprentissage s√©curis√© avec transactions

```normil
@atomic
@plastic(rate: 0.005, mode: "hebbian")
fn apprentissage_securise(poids: Vec, entree: Vec, seuil: float) -> Vec {
    // Calcul plastique
    let delta = scale(vec_mul(poids, entree), 0.005)
    let nouveau = vec_add(poids, delta)
  
    // V√©rification de stabilit√©
    let n = norm(nouveau)
  
    if n > seuil {
        // Trop instable - rollback
        return poids
    }
  
    return normalize(nouveau)
}

fn main() {
    let poids = random(dim: 64, mean: 0.0, std: 0.1)
    let signal_normal = random(dim: 64, mean: 0.5, std: 0.1)
    let signal_fort = random(dim: 64, mean: 5.0, std: 2.0)
  
    print("Norme initiale:")
    print(norm(poids))
  
    // Signal normal - devrait fonctionner
    poids = apprentissage_securise(poids, signal_normal, seuil: 2.0)
    print("Apr√®s signal normal:")
    print(norm(poids))
  
    // Signal trop fort - rollback
    poids = apprentissage_securise(poids, signal_fort, seuil: 2.0)
    print("Apr√®s signal fort (rollback):")
    print(norm(poids))
}
```

**Exercice** : Ajoutez des seuils min et max pour la stabilit√©.

---

## Niveau 4 : Expert

### Projet 4.1 : R√©seau de Neurones Simple

**Objectif** : Impl√©menter un perceptron avec apprentissage

```normil
@plastic(rate: 0.001, mode: "backprop")
fn backprop_update(poids: Vec, gradient: Vec) -> Vec {
    let correction = scale(gradient, 0.001)
    let nouveau = vec_sub(poids, correction)
    return normalize(nouveau)
}

@atomic
fn forward_propagation(entree: Vec, poids: Vec) -> Vec {
    let weighted = vec_mul(entree, poids)
    return normalize(weighted)
}

fn calculer_erreur(sortie: Vec, cible: Vec) -> Vec {
    return vec_sub(cible, sortie)
}

fn entrainer_reseau(
    poids_init: Vec,
    entrees: Vec,
    cibles: Vec,
    epochs: int
) -> Vec {
    let poids = poids_init
  
    for epoch in range(0, epochs) {
        // Forward
        let sortie = forward_propagation(entrees, poids)
      
        // Calcul erreur
        let erreur = calculer_erreur(sortie, cibles)
      
        // Backward
        poids = backprop_update(poids, erreur)
      
        if epoch % 10 == 0 {
            print("Epoch")
            print(epoch)
            print("Erreur:")
            print(norm(erreur))
        }
    }
  
    return poids
}

fn main() {
    let dim = 128
  
    let poids = random(dim: dim, mean: 0.0, std: 0.1)
    let entree = random(dim: dim, mean: 1.0, std: 0.2)
    let cible = random(dim: dim, mean: 0.5, std: 0.1)
  
    print("Entrainement...")
    let poids_entraines = entrainer_reseau(
        poids_init: poids,
        entrees: entree,
        cibles: cible,
        epochs: 50
    )
  
    print("Entrainement termine!")
    print("Norme finale:")
    print(norm(poids_entraines))
}
```

---

### Projet 4.2 : Syst√®me de M√©moire avec Consolidation

**Objectif** : Impl√©menter consolidation + oubli

```normil
@plastic(rate: 0.02, mode: "hebbian")
fn encoder_memoire(memoire: Vec, pattern: Vec) -> Vec {
    let association = vec_mul(memoire, pattern)
    let renforcement = scale(association, 0.02)
    return vec_add(memoire, renforcement)
}

@plastic(rate: 0.005, mode: "anti_hebbian")
fn oubli_progressif(memoire: Vec, bruit: Vec) -> Vec {
    let decay = scale(vec_mul(memoire, bruit), 0.005)
    return vec_sub(memoire, decay)
}

@atomic
fn consolider_memoire(memoire: Vec) -> Vec {
    let n = norm(memoire)
  
    if n < 0.1 {
        // M√©moire trop faible - r√©initialiser
        return zeros(dim: 64)
    }
  
    if n > 2.0 {
        // Trop forte - normaliser
        return normalize(memoire)
    }
  
    return memoire
}

fn cycle_memoire(
    memoire_init: Vec,
    patterns: int,
    cycles: int
) -> Vec {
    let memoire = memoire_init
  
    for cycle in range(0, cycles) {
        // Encoder nouveau pattern
        let pattern = random(dim: 64, mean: 1.0, std: 0.2)
        memoire = encoder_memoire(memoire, pattern)
      
        // Oubli avec bruit
        let bruit = random(dim: 64, mean: 0.5, std: 0.1)
        memoire = oubli_progressif(memoire, bruit)
      
        // Consolidation
        memoire = consolider_memoire(memoire)
      
        print("Cycle")
        print(cycle)
        print("Norme:")
        print(norm(memoire))
    }
  
    return memoire
}

fn main() {
    let memoire = zeros(dim: 64)
  
    print("Simulation memoire...")
    let memoire_finale = cycle_memoire(
        memoire_init: memoire,
        patterns: 10,
        cycles: 20
    )
  
    print("Simulation terminee!")
}
```

---

### Projet 4.3 : D√©tecteur de Patterns avec Classification

**Objectif** : Combiner patterns, @plastic, @atomic

```normil
fn classifier_force(norme: float) -> str {
    match norme {
        case float(n) where n > 3.0 -> { return "tres fort" }
        case float(n) where n > 2.0 -> { return "fort" }
        case float(n) where n > 1.0 -> { return "moyen" }
        case float(n) where n > 0.5 -> { return "faible" }
        case _ -> { return "tres faible" }
    }
}

@plastic(rate: 0.01, mode: "competitive")
fn adapter_detecteur(detecteur: Vec, signal: Vec) -> Vec {
    let reponse = vec_mul(detecteur, signal)
    let adaptation = scale(reponse, 0.01)
    return vec_add(detecteur, adaptation)
}

@atomic
fn detecter_pattern(detecteur: Vec, signal: Vec, seuil: float) -> bool {
    let activation = dot(detecteur, signal)
  
    if activation > seuil {
        return true
    } else {
        return false
    }
}

fn entrainer_detecteur(
    detecteur_init: Vec,
    signaux_positifs: int,
    signaux_negatifs: int
) -> Vec {
    let detecteur = detecteur_init
  
    print("Phase 1: Apprentissage patterns positifs")
    for i in range(0, signaux_positifs) {
        let signal_pos = random(dim: 64, mean: 2.0, std: 0.3)
        detecteur = adapter_detecteur(detecteur, signal_pos)
      
        let classe = classifier_force(norm(detecteur))
        print(classe)
    }
  
    print("Phase 2: Adaptation patterns negatifs")
    for i in range(0, signaux_negatifs) {
        let signal_neg = random(dim: 64, mean: 0.2, std: 0.1)
        let inverse = scale(signal_neg, -0.5)
        detecteur = adapter_detecteur(detecteur, inverse)
      
        let classe = classifier_force(norm(detecteur))
        print(classe)
    }
  
    return normalize(detecteur)
}

fn main() {
    let detecteur = random(dim: 64, mean: 0.0, std: 0.1)
  
    detecteur = entrainer_detecteur(
        detecteur_init: detecteur,
        signaux_positifs: 10,
        signaux_negatifs: 5
    )
  
    print("Test de detection:")
    let test_signal = random(dim: 64, mean: 1.5, std: 0.2)
    let detected = detecter_pattern(detecteur, test_signal, seuil: 50.0)
  
    print("Pattern detecte:")
    print(detected)
}
```

---

## Projets Complets

### Projet Final 1 : Syst√®me d'Apprentissage Multi-Couches

```normil
// Couche 1: Encodage
@plastic(rate: 0.01, mode: "hebbian")
fn couche_encodage(entree: Vec, poids: Vec) -> Vec {
    let code = vec_mul(entree, poids)
    return normalize(code)
}

// Couche 2: Traitement
@atomic
@plastic(rate: 0.005, mode: "stdp")
fn couche_traitement(code: Vec, poids: Vec) -> Vec {
    let traite = vec_mul(code, poids)
    let n = norm(traite)
  
    if n > 5.0 {
        return normalize(traite)
    }
  
    return traite
}

// Couche 3: Sortie
@atomic
fn couche_sortie(traite: Vec, poids: Vec, seuil: float) -> str {
    let sortie = dot(traite, poids)
  
    match sortie {
        case float(s) where s > seuil * 2.0 -> {
            return "Classe A"
        }
        case float(s) where s > seuil -> {
            return "Classe B"
        }
        case _ -> {
            return "Classe C"
        }
    }
}

fn reseau_complet(
    entree: Vec,
    poids1: Vec,
    poids2: Vec,
    poids3: Vec
) -> str {
    let code = couche_encodage(entree, poids1)
    let traite = couche_traitement(code, poids2)
    let classe = couche_sortie(traite, poids3, seuil: 25.0)
    return classe
}

fn main() {
    let dim = 128
  
    let p1 = random(dim: dim, mean: 0.0, std: 0.1)
    let p2 = random(dim: dim, mean: 0.0, std: 0.1)
    let p3 = random(dim: dim, mean: 0.0, std: 0.1)
  
    print("Test 1:")
    let e1 = random(dim: dim, mean: 3.0, std: 0.5)
    print(reseau_complet(e1, p1, p2, p3))
  
    print("Test 2:")
    let e2 = random(dim: dim, mean: 1.0, std: 0.2)
    print(reseau_complet(e2, p1, p2, p3))
  
    print("Test 3:")
    let e3 = random(dim: dim, mean: 0.1, std: 0.05)
    print(reseau_complet(e3, p1, p2, p3))
}
```

---

## Exercices de Synth√®se

### Exercice Avanc√© 1

Cr√©ez un syst√®me de reconnaissance de patterns avec:

- 3 types de patterns diff√©rents
- Apprentissage @plastic avec mode au choix
- Validation @atomic des r√©sultats
- Classification par pattern matching

### Exercice Avanc√© 2

Impl√©mentez une m√©moire associative avec:

- Stockage de 5 patterns
- Rappel par similarit√©
- Consolidation progressive
- Oubli contr√¥l√©

### Exercice Avanc√© 3

D√©veloppez un r√©seau comp√©titif avec:

- Plusieurs neurones en comp√©tition
- Apprentissage winner-take-all
- Stabilisation @atomic
- Analyse des clusters form√©s

---

## Niveau 5 : Types O-RedMind (‚ú® Phase 5)

### Le√ßon 5.1 : EpisodicRecord - M√©moire √âpisodique

**Objectif** : Stocker des √©v√©nements bruts horodat√©s avec vecteurs multimodaux

```normil
fn main() {
    // Cr√©ation d'un enregistrement √©pisodique
    let memory = EpisodicRecord {
        id: "event_001",
        timestamp: 1698000000.0,
        sources: ["camera", "audio"],
        vecs: {},
        summary: "User said hello",
        labels: [],
        trust: 0.95,
        provenance: {},
        outcome: "success"
    }
  
    // Acc√®s aux champs
    print("Event ID: " + memory.id)
    print("Trust: " + to_string(memory.trust))
    print("Summary: " + memory.summary)
  
    // Modification
    memory.outcome = "completed"
    memory.trust = 0.98
}
```

**Cas d'usage** : Journalisation d'√©v√©nements, tra√ßabilit√©, analyse comportementale

---

### Le√ßon 5.2 : Concept - M√©moire S√©mantique

**Objectif** : Repr√©senter des concepts compress√©s avec confiance

```normil
fn main() {
    // Cr√©ation d'un concept
    let ai_concept = Concept {
        concept_id: "ai_ml_001",
        centroid_vec: vec(128, [1.0, 0.5, -0.3, 0.8]),
        doc_count: 42,
        provenance_versions: ["v1.0", "v1.1"],
        trust_score: 0.85,
        labels: ["AI", "machine_learning", "neural_networks"]
    }
  
    // Acc√®s et modification
    print("Concept: " + ai_concept.concept_id)
    print("Documents: " + to_string(ai_concept.doc_count))
    print("Trust: " + to_string(ai_concept.trust_score))
  
    // Mettre √† jour apr√®s apprentissage
    ai_concept.doc_count = ai_concept.doc_count + 10
    ai_concept.trust_score = 0.90
}
```

**Cas d'usage** : Knowledge base, clustering s√©mantique, compression d'information

---

### Le√ßon 5.3 : ProtoInstinct - Instincts Prototypiques

**Objectif** : D√©finir des comportements instinctifs avec vecteurs de r√©f√©rence

```normil
fn main() {
    // Cr√©ation d'un proto-instinct
    let safety_instinct = ProtoInstinct {
        id: "privacy_guard",
        vec_ref: vec(64, [0.8, 0.9, 0.7, 0.95]),
        rule: "if similarity > 0.9 then activate",
        weight: 1.5
    }
  
    // Utilisation dans une fonction
    fn should_activate(instinct: ProtoInstinct, threshold: float) -> bool {
        if instinct.weight > threshold {
            return true
        }
        return false
    }
  
    let active = should_activate(safety_instinct, 1.0)
    print("Instinct actif: " + to_string(active))
  
    // Ajustement dynamique
    safety_instinct.weight = 2.0
}
```

**Cas d'usage** : Syst√®mes de s√©curit√©, comportements r√©actifs, priorit√©s dynamiques

---

### Le√ßon 5.4 : SparseVec - Vecteurs Creux Optimis√©s

**Objectif** : Stocker efficacement des vecteurs avec beaucoup de z√©ros

```normil
fn main() {
    // Cr√©ation d'un vecteur creux
    // Seulement 5 valeurs non-nulles sur 1000 dimensions
    let sparse = SparseVec {
        indices: [0, 100, 250, 500, 999],
        values: [1.5, 2.0, -0.5, 3.0, 0.8],
        dim: 1000
    }
  
    print("Dimension: " + to_string(sparse.dim))
    print("Non-zeros: " + to_string(len(sparse.indices)))
  
    // Calcul de sparsit√©
    fn sparsity(sv: SparseVec) -> float {
        let nnz = len(sv.indices)
        return (1.0 - (to_float(nnz) / to_float(sv.dim))) * 100.0
    }
  
    let sp = sparsity(sparse)
    print("Sparsit√©: " + to_string(sp) + "%")
  
    // Liste de vecteurs creux
    let sparse_list = [
        SparseVec {
            indices: [0, 1],
            values: [1.0, 2.0],
            dim: 100
        },
        SparseVec {
            indices: [50, 99],
            values: [0.5, 0.8],
            dim: 100
        }
    ]
  
    print("Nombre de vecteurs: " + to_string(len(sparse_list)))
}
```

**Cas d'usage** : NLP (word embeddings), r√©seaux de neurones creux, √©conomie m√©moire

---

### Le√ßon 5.5 : Combinaison des Types O-RedMind

**Objectif** : Utiliser tous les types ensemble pour un syst√®me complet

```normil
// Syst√®me de m√©moire intelligent
fn systeme_memoire() {
    // M√©moire √©pisodique
    let events = [
        EpisodicRecord {
            id: "e001",
            timestamp: 1698000000.0,
            sources: ["sensor"],
            vecs: {},
            summary: "Temperature spike detected",
            labels: [],
            trust: 0.9,
            provenance: {},
            outcome: "analyzed"
        }
    ]
  
    // Concepts appris
    let concepts = [
        Concept {
            concept_id: "temperature_anomaly",
            centroid_vec: vec(64, [0.9, 0.8, 0.7, 0.95]),
            doc_count: 15,
            provenance_versions: ["v1"],
            trust_score: 0.88,
            labels: ["anomaly", "temperature"]
        }
    ]
  
    // Instincts de s√©curit√©
    let instincts = [
        ProtoInstinct {
            id: "alert_system",
            vec_ref: vec(64, [0.85, 0.9, 0.75, 0.92]),
            rule: "if trust > 0.85 then alert",
            weight: 2.0
        }
    ]
  
    // Repr√©sentation creuse
    let feature_vec = SparseVec {
        indices: [5, 12, 28, 45],
        values: [1.0, 0.8, 0.6, 0.9],
        dim: 64
    }
  
    print("Syst√®me de m√©moire initialis√©")
    print("Events: " + to_string(len(events)))
    print("Concepts: " + to_string(len(concepts)))
    print("Instincts: " + to_string(len(instincts)))
    print("Features sparsity: " + to_string(len(feature_vec.indices)) + "/" + to_string(feature_vec.dim))
}

fn main() {
    systeme_memoire()
}
```

**Cas d'usage** : Agents intelligents, syst√®mes de m√©moire hi√©rarchique, IA contextuelle

---

### Exemples Complets Phase 5

Consultez les fichiers d'exemples dans `examples/` :

- `test_episodic_record.nor` - Tous les cas d'usage EpisodicRecord
- `test_concept_simple.nor` - Manipulation de Concepts
- `test_protoinstinct_simple.nor` - Gestion d'instincts
- `test_sparsevec_simple.nor` - Vecteurs creux optimis√©s

---

## Niveau 6 : Primitives Neurales & Transactions

La Phase 6 introduit des primitives neurales avanc√©es et un syst√®me de transactions avec audit automatique.

### Le√ßon 6.1 : Low-Rank Update

La primitive `lowrankupdate()` permet de mettre √† jour une matrice de mani√®re efficace avec un produit ext√©rieur de rang 1.

**Formule** : W' = W + u ‚äó v

```normil
// Mise √† jour de rang faible
let W = [[1.0, 0.0], [0.0, 1.0]]  // Matrice identit√©
let u = vec(2, [1.0, 0.0])
let v = vec(2, [0.0, 1.0])

// Ajouter u‚äóv √† W
let W_new = lowrankupdate(W, u, v)
// R√©sultat: [[1.0, 1.0], [0.0, 1.0]]
```

**Cas d'usage** :

- Adaptation de poids neuronaux sans r√©-entra√Ænement complet
- Apprentissage incr√©mental
- Mise √† jour de mod√®les avec faible co√ªt computationnel

---

### Le√ßon 6.2 : Quantization

La primitive `quantize()` compresse un vecteur en r√©duisant sa pr√©cision √† 8 ou 4 bits.

```normil
// Quantisation pour √©conomie m√©moire
let v = random(128, 0.0, 1.0)

// Quantisation 8 bits (haute pr√©cision)
let v_q8 = quantize(v, 8)

// Quantisation 4 bits (haute compression)
let v_q4 = quantize(v, 4)

// La dimension est pr√©serv√©e
print(v.dim)      // 128
print(v_q8.dim)   // 128
print(v_q4.dim)   // 128
```

**Comparaison** :

- **8-bit** : ~1% d'erreur, 50% de compression
- **4-bit** : ~5% d'erreur, 75% de compression

**Cas d'usage** :

- Stockage de vecteurs en production
- Transmission r√©seau optimis√©e
- Syst√®mes embarqu√©s avec m√©moire limit√©e

---

### Le√ßon 6.3 : Online Clustering

La primitive `onlinecluster_update()` met √† jour un centro√Øde de mani√®re incr√©mentale.

**Formule** : c' = (1 - lr) √ó c + lr √ó x

```normil
// Clustering en ligne
let centroid = zeros(64)
let lr = 0.1  // Learning rate

// Ajouter progressivement des points
let x1 = random(64, 0.0, 1.0)
centroid = onlinecluster_update(centroid, x1, lr)

let x2 = random(64, 0.0, 1.0)
centroid = onlinecluster_update(centroid, x2, lr)

let x3 = random(64, 0.0, 1.0)
centroid = onlinecluster_update(centroid, x3, lr)

// Le centro√Øde converge vers la moyenne des points
```

**Param√®tre learning rate** :

- `lr = 0.0` : Aucun changement
- `lr = 0.1` : Adaptation lente, stable
- `lr = 0.5` : Adaptation moyenne
- `lr = 1.0` : Remplacement complet

**Cas d'usage** :

- Consolidation s√©mantique en temps r√©el
- Clustering sans stocker tous les points
- Adaptation continue de concepts

---

### Le√ßon 6.4 : Syst√®me de Transactions

Les transactions garantissent la tra√ßabilit√© et l'int√©grit√© des op√©rations critiques avec audit logging automatique.

```normil
// D√©claration d'une transaction
transaction append_episode_safe(summary: str, trust: float) -> str {
    // Cr√©er un enregistrement √©pisodique
    let v = random(128, 0.0, 1.0)
    let record = EpisodicRecord {
        id: generate_uuid(),
        timestamp: now(),
        sources: ["system"],
        vecs: {"default": v},
        summary: summary,
        labels: [],
        trust: trust,
        provenance: {"device_id": "prod", "signature": ""},
        outcome: "success"
    }
  
    // Cette op√©ration est automatiquement logg√©e
    let id = episodic_append(record)
  
    return id
}

// Appel de la transaction
let episode_id = append_episode_safe("Important event", 0.95)
```

**Avantages** :

- ‚úÖ **Audit automatique** : Chaque transaction est logg√©e (start/success/failed)
- ‚úÖ **Tra√ßabilit√©** : Horodatage et param√®tres enregistr√©s
- ‚úÖ **Rollback** : En cas d'erreur, √©tat restaur√© automatiquement
- ‚úÖ **Int√©grit√©** : Hash chaining pour v√©rification

**Transaction avec rollback** :

```normil
transaction update_concept(concept_id: str, new_vec: Vec) {
    let old = semantic_query(concept_id, k: 1)[0]
    semantic_upsert(concept_id, new_vec)
    audit_log("concept_updated", concept_id)
} rollback {
    // En cas d'erreur, restaurer l'ancien vecteur
    semantic_upsert(concept_id, old.centroid_vec)
}
```

---

### Le√ßon 6.5 : Exemple Complet - Syst√®me d'Apprentissage

Combinons toutes les primitives neurales dans un syst√®me complet :

```normil
// Syst√®me d'apprentissage incr√©mental avec transactions
transaction learn_from_experience(input: Vec, label: str, trust: float) -> str {
    // 1. Quantifier pour √©conomie m√©moire
    let input_q = quantize(input, 8)
  
    // 2. Chercher le concept le plus proche
    let similar = semantic_query(input_q, k: 1)
  
    let concept_id = ""
  
    if len(similar) > 0 {
        // Concept existant : mise √† jour incr√©mentale
        let existing = similar[0]
        concept_id = existing.concept_id
      
        // Mettre √† jour le centro√Øde
        let new_centroid = onlinecluster_update(
            existing.centroid_vec,
            input_q,
            0.1
        )
      
        // Low-rank update pour affiner
        let u = input_q
        let v = existing.centroid_vec
        let refined = vec(input.dim, [0.0])  // Placeholder pour matrice
      
        // Sauvegarder le concept mis √† jour
        let updated = Concept {
            concept_id: concept_id,
            centroid_vec: new_centroid,
            doc_count: existing.doc_count + 1,
            provenance_versions: existing.provenance_versions,
            trust_score: (existing.trust_score + trust) / 2.0,
            labels: existing.labels + [label]
        }
      
        semantic_upsert(updated)
      
    } else {
        // Nouveau concept
        concept_id = generate_uuid()
      
        let new_concept = Concept {
            concept_id: concept_id,
            centroid_vec: input_q,
            doc_count: 1,
            provenance_versions: [],
            trust_score: trust,
            labels: [label]
        }
      
        semantic_upsert(new_concept)
    }
  
    // 3. Enregistrer l'√©pisode
    let record = EpisodicRecord {
        id: generate_uuid(),
        timestamp: now(),
        sources: ["learning_system"],
        vecs: {"input": input_q},
        summary: "Learned: " + label,
        labels: [{"label": label, "score": trust}],
        trust: trust,
        provenance: {"device_id": "learner", "signature": ""},
        outcome: "learned"
    }
  
    episodic_append(record)
  
    return concept_id
}

// Utilisation
let v1 = random(128, 0.0, 1.0)
let c1 = learn_from_experience(v1, "concept_A", 0.9)

let v2 = random(128, 0.0, 1.0)
let c2 = learn_from_experience(v2, "concept_A", 0.85)

print("Learned concept: " + c1)
```

**Ce syst√®me** :

- ‚úÖ Quantifie les entr√©es (√©conomie m√©moire)
- ‚úÖ Clustering incr√©mental (pas de r√©-entra√Ænement)
- ‚úÖ Low-rank updates (adaptation fine)
- ‚úÖ Transactions audit√©es (tra√ßabilit√© compl√®te)
- ‚úÖ M√©moire √©pisodique + s√©mantique (O-RedMind complet)

---

## Conclusion

Vous ma√Ætrisez maintenant :
‚úÖ Les bases de NORMiL (variables, fonctions, conditions, boucles)
‚úÖ **Inf√©rence de types** automatique (Phase 3.1)
‚úÖ **Op√©rations sur cha√Ænes** et concat√©nation (Phase 3.3)
‚úÖ Les vecteurs et op√©rations vectorielles
‚úÖ **Syst√®me de modules** et imports (Phase 3.2)
‚úÖ **Interop√©rabilit√© Python compl√®te** (Phase 4) :

- Import de modules Python (Phase 4.1)
- Appel de fonctions Python (Phase 4.2)
- Conversions de types automatiques (Phase 4.3)
- Acc√®s aux objets, classes et m√©thodes Python (Phase 4.4)
  ‚úÖ **Types O-RedMind sp√©cialis√©s** (Phase 5) :
- EpisodicRecord : m√©moire √©pisodique horodat√©e
- Concept : m√©moire s√©mantique compress√©e
- ProtoInstinct : comportements instinctifs
- SparseVec : vecteurs creux optimis√©s
  ‚úÖ **Primitives neurales & transactions** (Phase 6) :
- lowrankupdate() : Mise √† jour de rang faible W' = W + u‚äóv
- quantize() : Quantisation 8/4 bits pour compression
- onlinecluster_update() : Clustering incr√©mental
- transaction : Syst√®me avec audit logging automatique

---

## Niveau 7 : Plasticit√© Neuronale Avanc√©e

### Le√ßon 7.1 - Annotation @plastic avec Stabilit√©

L'annotation `@plastic` peut maintenant d√©tecter automatiquement quand une fonction atteint la stabilit√© :

```normil
@plastic(rate: 0.01, mode: "hebbian", stability_threshold: 0.01)
fn learn_pattern(input: Vec, target: Vec) -> Vec {
    // La plasticit√© s'arr√™te automatiquement quand stable
    let weights = hebbian_update(input, target, 0.01)
    return weights
}
```

**Param√®tres** :

- `rate` : Taux d'apprentissage initial (d√©cro√Æt automatiquement)
- `mode` : Type de plasticit√© (`"hebbian"`, `"stdp"`, `"anti_hebbian"`)
- `stability_threshold` : Seuil de convergence (d√©faut: 0.01 = 1%)

**M√©tadonn√©es automatiques** :

- `step_count` : Nombre d'appels √† la fonction
- `is_stable` : True quand la stabilit√© est atteinte
- Learning rate d√©cro√Æt automatiquement jusqu'√† stabilit√©

### Le√ßon 7.2 - Modes de Plasticit√©

Trois modes impl√©ment√©s avec normalisation automatique :

```normil
// Mode Hebbian : Renforcement corr√©l√©
@plastic(rate: 0.005, mode: "hebbian")
fn hebbian_learn(pre: Vec, post: Vec) -> Vec {
    let weights = outer_product(pre, post)
    return weights  // Automatiquement normalis√© (norme L2 = 1.0)
}

// Mode STDP : Spike-Timing Dependent Plasticity
@plastic(rate: 0.01, mode: "stdp")
fn stdp_learn(spike_train: Vec, timing: Vec) -> Vec {
    let weights = time_dependent_update(spike_train, timing)
    return weights  // Auto-normalis√©
}

// Mode Anti-Hebbian : D√©corr√©lation
@plastic(rate: 0.003, mode: "anti_hebbian")
fn anti_hebbian_learn(pattern: Vec) -> Vec {
    let weights = decorrelate(pattern)
    return weights  // Auto-normalis√©
}
```

**Caract√©ristiques communes** :

- ‚úÖ Normalisation L2 automatique des r√©sultats Vec
- ‚úÖ Decay du learning rate quand non-stable
- ‚úÖ D√©tection de convergence automatique

### Le√ßon 7.3 - Primitives de Gestion de Plasticit√©

#### normalize_plasticity()

Normalise un vecteur √† norme L2 = 1.0 :

```normil
let weights = vec(3, [3.0, 4.0, 0.0])
let normalized = normalize_plasticity(weights)
// normalized = [0.6, 0.8, 0.0], norme = 1.0

print("Norme: " + to_string(norm(normalized)))  // 1.0
```

**Utilisation** : Maintenir la magnitude constante pendant l'apprentissage.

#### decay_learning_rate()

D√©croissance exponentielle du taux d'apprentissage :

```normil
let lr = 0.1
let factor = 0.95  // D√©croissance de 5% par √©tape

lr = decay_learning_rate(lr, factor)
// lr = 0.095

// Apr√®s 10 √©tapes
for i in range(10) {
    lr = decay_learning_rate(lr, 0.95)
}
// lr ‚âà 0.0599
```

**Utilisation** : Convergence progressive vers un optimum.

#### compute_stability()

V√©rifie si deux vecteurs sont stables (changement relatif < seuil) :

```normil
let w_old = vec(3, [1.0, 2.0, 3.0])
let w_new = vec(3, [1.001, 2.002, 3.001])

let is_stable = compute_stability(w_old, w_new, 0.01)
// is_stable = true (changement < 1%)

let w_diff = vec(3, [1.5, 3.0, 4.5])
let is_unstable = compute_stability(w_old, w_diff, 0.01)
// is_unstable = false (changement ‚âà 50%)
```

**Utilisation** : Crit√®re d'arr√™t pour l'apprentissage.

### Le√ßon 7.4 - Gestion Automatique de la Plasticit√©

Les fonctions `@plastic` b√©n√©ficient d'une gestion automatique compl√®te :

```normil
@plastic(rate: 0.1, mode: "hebbian", stability_threshold: 0.005)
fn adaptive_network(input: Vec) -> Vec {
    // Variables "weights", "w", "synapses" ou "connections" 
    // sont automatiquement track√©es
    let weights = random_vec(input.dim)
  
    // Traitement
    weights = onlinecluster_update(weights, input, 0.1)
  
    return weights
    // √Ä chaque appel :
    // 1. step_count++
    // 2. V√©rification stabilit√© (si weights captur√©s)
    // 3. Normalisation automatique (mode hebbian/stdp/anti_hebbian)
    // 4. Decay du LR (si non-stable)
}

// Utilisation
let data = vec(10, [0.5, 0.3, ...])
let learned1 = adaptive_network(data)  // step 1, LR=0.1
let learned2 = adaptive_network(data)  // step 2, LR‚âà0.099
let learned3 = adaptive_network(data)  // step 3, LR‚âà0.098
// ... convergence automatique
```

**B√©n√©fices** :

- ‚úÖ Z√©ro code boilerplate pour la plasticit√©
- ‚úÖ Convergence garantie (via decay + stabilit√©)
- ‚úÖ Poids toujours normalis√©s
- ‚úÖ Tra√ßabilit√© compl√®te (step_count, is_stable)

### Le√ßon 7.5 - Sc√©nario Complet : Apprentissage Multi-Couches

```normil
@plastic(rate: 0.05, mode: "hebbian", stability_threshold: 0.01)
fn layer1(input: Vec) -> Vec {
    let w = zeros(input.dim)
    w = onlinecluster_update(w, input, 0.05)
    return w  // Auto-normalis√©
}

@plastic(rate: 0.03, mode: "stdp", stability_threshold: 0.005)
fn layer2(hidden: Vec) -> Vec {
    let w = zeros(hidden.dim)
    w = onlinecluster_update(w, hidden, 0.03)
    return w  // Auto-normalis√©
}

fn train_network(data: Vec) {
    // Couche 1
    let hidden = layer1(data)
    print("Hidden norm: " + to_string(norm(hidden)))  // ‚âà1.0
  
    // Couche 2
    let output = layer2(hidden)
    print("Output norm: " + to_string(norm(output)))  // ‚âà1.0
  
    // Chaque couche converge ind√©pendamment
}

// Entra√Ænement progressif
let training_data = vec(20, [...])
for epoch in range(100) {
    train_network(training_data)
    // Convergence automatique de chaque couche
}
```

**R√©sultat** :

- Chaque couche apprend son niveau de repr√©sentation
- Normalisation garantit la stabilit√© num√©rique
- Convergence d√©tect√©e automatiquement
- Pas de r√©glage manuel des hyperparam√®tres

### Le√ßon 7.6 - Combinaison avec Transactions

Plasticit√© + Transactions = Apprentissage tra√ßable :

```normil
@atomic
@plastic(rate: 0.02, mode: "hebbian")
fn safe_learn(pattern: Vec, label: string) -> Vec {
    transaction {
        audit("Learning pattern: " + label)
      
        let weights = zeros(pattern.dim)
        weights = onlinecluster_update(weights, pattern, 0.02)
      
        audit("Weights norm: " + to_string(norm(weights)))
      
        return weights  // Auto-normalis√© + logged
    }
}

// En cas d'erreur, rollback automatique
// Chaque √©tape d'apprentissage est trac√©e
```

**Avantages** :

- üîç Tra√ßabilit√© compl√®te de l'apprentissage
- üîÑ Rollback en cas de probl√®me
- üìä Audit logging automatique
- ‚úÖ Convergence garantie

### Le√ßon 7.7 - Modes de Plasticit√© Personnalis√©s

Cr√©ez vos propres modes d'apprentissage au-del√† de hebbian/stdp/anti_hebbian :

```normil
// Enregistrer un nouveau mode
let oja_registered = register_plasticity_mode(
    "oja",           // Nom du mode
    true,            // Auto-normaliser ?
    "Oja's learning rule"  // Description
)

// Utiliser le mode personnalis√©
@plastic(rate: 0.01, mode: "oja")
fn oja_network(input: Vec) -> Vec {
    let w = zeros(input.dim)
    w = onlinecluster_update(w, input, 0.01)
    return w  // Normalis√© automatiquement car normalize=true
}

// Lister tous les modes disponibles
let all_modes = list_plasticity_modes()
// ["hebbian", "stdp", "anti_hebbian", "oja", ...]
print("Available modes: " + to_string(len(all_modes)))
```

**Cas d'usage** :

- Impl√©menter des r√®gles d'apprentissage sp√©cifiques
- Contr√¥ler finement la normalisation
- Organiser des exp√©riences comparatives

### Le√ßon 7.8 - Decay Factor Configurable

Contr√¥lez la vitesse de d√©croissance du learning rate :

```normil
// Decay rapide (convergence rapide mais moins pr√©cise)
@plastic(rate: 0.1, decay_factor: 0.90)
fn fast_learner(data: Vec) -> Vec {
    let w = zeros(data.dim)
    w = onlinecluster_update(w, data, 0.1)
    return w
    // LR d√©cro√Æt vite : 0.1 ‚Üí 0.09 ‚Üí 0.081 ‚Üí ...
}

// Decay lent (convergence lente mais tr√®s pr√©cise)
@plastic(rate: 0.1, decay_factor: 0.995)
fn precise_learner(data: Vec) -> Vec {
    let w = zeros(data.dim)
    w = onlinecluster_update(w, data, 0.1)
    return w
    // LR d√©cro√Æt lentement : 0.1 ‚Üí 0.0995 ‚Üí 0.099 ‚Üí ...
}

// Pas de decay (LR constant)
@plastic(rate: 0.1, decay_factor: 1.0)
fn constant_learner(data: Vec) -> Vec {
    let w = zeros(data.dim)
    w = onlinecluster_update(w, data, 0.1)
    return w
    // LR reste √† 0.1
}
```

**Strat√©gies** :

- `0.90-0.95` : Apprentissage rapide, exploration large
- `0.95-0.99` : √âquilibre (d√©faut: 0.99)
- `0.99-0.999` : Convergence fine, pr√©cision maximale
- `1.0` : LR constant (pas de decay)

### Le√ßon 7.9 - Multi-Crit√®res de Stabilit√©

D√©tection avanc√©e de convergence avec plusieurs crit√®res :

```normil
// Maintenir un historique des poids
let weight_history = []

for epoch in range(20) {
    let w = train_step(data)
    weight_history = weight_history + [w]
  
    // Crit√®re 1: Stabilit√© sur fen√™tre (tous les changements < seuil)
    let window_stable = compute_stability_window(weight_history, 0.01)
  
    // Crit√®re 2: Variance faible
    let variance = compute_weight_variance(weight_history)
    let var_stable = variance < 0.001
  
    // Convergence si TOUS les crit√®res sont satisfaits
    let converged = window_stable
    if converged {
        converged = var_stable
    }
  
    if converged {
        print("Convergence d√©tect√©e √† epoch " + to_string(epoch))
        break
    }
}
```

**Avantages** :

- D√©tection robuste (√©vite les faux positifs)
- Crit√®res compl√©mentaires (stabilit√© locale + globale)
- Arr√™t pr√©coce intelligent

### Le√ßon 7.10 - Scheduling du Learning Rate

Contr√¥le fin du LR avec diff√©rentes strat√©gies :

#### Warmup Lin√©aire

```normil
fn train_with_warmup(data: Vec, epochs: int) {
    let weights = zeros(data.dim)
    let warmup_steps = 10
    let target_lr = 0.01
  
    for epoch in range(epochs) {
        // Calculer LR avec warmup
        let current_lr = lr_warmup_linear(epoch, warmup_steps, target_lr)
      
        // Entra√Æner avec ce LR
        weights = onlinecluster_update(weights, data, current_lr)
      
        print("Epoch " + to_string(epoch) + ", LR: " + to_string(current_lr))
    }
}
// Epoch 0, LR: 0.0
// Epoch 5, LR: 0.005
// Epoch 10+, LR: 0.01
```

#### Cosine Annealing

```normil
fn train_with_cosine(data: Vec, total_epochs: int) {
    let weights = zeros(data.dim)
    let min_lr = 0.0001
    let max_lr = 0.01
  
    for epoch in range(total_epochs) {
        let current_lr = lr_cosine_annealing(epoch, total_epochs, min_lr, max_lr)
        weights = onlinecluster_update(weights, data, current_lr)
    }
    // LR d√©cro√Æt en cosinus: 0.01 ‚Üí ... ‚Üí 0.0001
}
```

#### Step Decay

```normil
fn train_with_steps(data: Vec, epochs: int) {
    let weights = zeros(data.dim)
    let initial_lr = 0.1
  
    for epoch in range(epochs) {
        // Diviser par 2 tous les 10 epochs
        let current_lr = lr_step_decay(epoch, initial_lr, 0.5, 10)
        weights = onlinecluster_update(weights, data, current_lr)
    }
    // Epochs 0-9: LR=0.1
    // Epochs 10-19: LR=0.05
    // Epochs 20-29: LR=0.025
}
```

#### Plateau Detection

```normil
fn train_with_plateau(data: Vec, epochs: int) {
    let weights = zeros(data.dim)
    let current_lr = 0.01
    let losses = []
  
    for epoch in range(epochs) {
        weights = onlinecluster_update(weights, data, current_lr)
      
        // Calculer loss
        let diff = data - weights
        let loss = dot(diff, diff)
        losses = losses + [loss]
      
        // R√©duire LR si plateau
        let reduction_factor = lr_plateau_factor(losses, 3, 0.5, 0.01)
        current_lr = current_lr * reduction_factor
    }
    // LR r√©duit automatiquement si pas d'am√©lioration
}
```

#### Combinaison Warmup + Cosine

```normil
fn advanced_scheduling(data: Vec, total_epochs: int) {
    let weights = zeros(data.dim)
    let warmup_steps = 10
  
    for epoch in range(total_epochs) {
        let current_lr = 0.0
      
        // Phase 1: Warmup
        if epoch < warmup_steps {
            current_lr = lr_warmup_linear(epoch, warmup_steps, 0.01)
        }
      
        // Phase 2: Cosine annealing
        if epoch >= warmup_steps {
            let adjusted_epoch = epoch - warmup_steps
            let adjusted_total = total_epochs - warmup_steps
            current_lr = lr_cosine_annealing(adjusted_epoch, adjusted_total, 0.0001, 0.01)
        }
      
        weights = onlinecluster_update(weights, data, current_lr)
    }
}
```

**Strat√©gies recommand√©es** :

- **Warmup + Cosine** : Meilleure performance g√©n√©rale
- **Step Decay** : Simple et efficace pour r√©seaux profonds
- **Plateau Detection** : Adaptatif, id√©al si incertitude sur dur√©e
- **Cosine seul** : Bon compromis convergence/simplicit√©

---

## Conclusion

F√©licitations ! Vous ma√Ætrisez maintenant **NORMiL v0.7.0** avec :

‚úÖ Les types de base et les op√©rations
‚úÖ Les structures de contr√¥le
‚úÖ Les fonctions et la r√©cursion
‚úÖ Les types O-RedMind avanc√©s :

- EpisodicRecord : M√©moire √©pisodique avec vecteurs multiples
- Concept : M√©moire s√©mantique avec centro√Ødes
- ProtoInstinct : Comportements instinctifs avec r√®gles
- SparseVec : Vecteurs creux optimis√©s
  ‚úÖ Les primitives neurales (Phase 6) :
- lowrankupdate() : Mises √† jour de rang faible
- quantize() : Quantisation 8/4 bits
- onlinecluster_update() : Clustering incr√©mental
- transaction : Syst√®me avec audit logging automatique
  ‚úÖ La plasticit√© neuronale avanc√©e (Phase 7) :
- @plastic avec d√©tection de stabilit√©
- Modes hebbian, stdp, anti_hebbian
- Primitives normalize_plasticity, decay_learning_rate, compute_stability
- Gestion automatique compl√®te
  ‚úÖ Am√©liorations avanc√©es de plasticit√© (Phase 7.6-7.9) :
- **Modes personnalisables** : register_plasticity_mode(), list_plasticity_modes()
- **Decay configurable** : decay_factor param√©trable (0.90-1.0)
- **Multi-crit√®res de stabilit√©** : compute_stability_window(), compute_weight_variance()
- **Scheduling du learning rate** :
  * lr_warmup_linear() : Warmup lin√©aire
  * lr_cosine_annealing() : D√©croissance cosinus
  * lr_step_decay() : Decay par paliers
  * lr_plateau_factor() : D√©tection de plateau
- **Op√©rations vectorielles** : +, -, * pour Vec
  ‚úÖ Les arguments nomm√©s
  ‚úÖ Le pattern matching complet
  ‚úÖ Les annotations @plastic et @atomic
  ‚úÖ La combinaison de toutes les features
  ‚úÖ La conception de syst√®mes complets

**Prochaines √©tapes** :

1. Explorez `examples/` pour plus d'inspiration :
   - `type_inference.nor` - D√©monstration d'inf√©rence
   - `imports_test.nor` - Utilisation de modules
   - `string_operations.nor` - Toutes les op√©rations string
   - `advanced_patterns.nor` - Pattern matching avanc√©
   - `neural_plasticity.nor` - Simulation compl√®te
   - `python_interop.nor` - Exemples d'int√©gration Python (modules, fonctions)
   - `python_objects.nor` - Utilisation d'objets Python (classes, m√©thodes)
   - `test_episodic_record.nor` - M√©moire √©pisodique (Phase 5)
   - `test_concept_simple.nor` - M√©moire s√©mantique (Phase 5)
   - `test_protoinstinct_simple.nor` - Instincts (Phase 5)
   - `test_sparsevec_simple.nor` - Vecteurs creux (Phase 5)
   - `test_neural_primitives.nor` - Primitives neurales (Phase 6)
   - `test_transactions.nor` - Syst√®me de transactions (Phase 6)
   - `test_plasticity_primitives.nor` - Primitives de plasticit√© (Phase 7)
   - `test_advanced_plasticity.nor` - Gestion automatique plasticit√© (Phase 7)
   - `test_custom_plasticity_modes.nor` - Modes personnalis√©s (Phase 7.6)
   - `test_decay_factor.nor` - Decay configurable (Phase 7.7)
   - `test_multi_criteria_stability.nor` - Stabilit√© multi-crit√®res (Phase 7.8)
   - `test_lr_scheduling.nor` - Scheduling du learning rate (Phase 7.9)
2. Cr√©ez vos propres modules r√©utilisables
3. Utilisez des biblioth√®ques Python (NumPy, SciPy, pandas, matplotlib, etc.)
4. Construisez des syst√®mes de m√©moire avec les types O-RedMind
5. Appliquez les primitives neurales pour l'apprentissage incr√©mental
6. Utilisez les transactions pour la tra√ßabilit√© critique
7. Exploitez la plasticit√© automatique pour l'apprentissage adaptatif
8. Consultez `API_REFERENCE.md` pour toutes les primitives
9. Consultez `PHASE2_FINAL_REPORT.md` pour les d√©tails Phase 2
10. Contribuez au projet (voir `CONTRIBUTING.md`)

**Bon coding avec NORMiL ! üöÄ**
